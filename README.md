# Buggi-2
Buggi-2, named after the "Kalki: 2898 AD" movie's Buggi character, is a language model trained on the Telugu language (CulturaX dataset on HF) using DGX (version 1 with relatively smaller data), and using A6000 (version 2 on CulturaX). Will host it on GCP if I get to use GCP's 300$ credit with GPU for some time.

### Sample results
#### 98M, Half of CulturaX `te` data:
```
Sample 1/3:
--------------------------------------------------
Generated: à°œà°—à°¨à± à°®à±‹à°¹à°¨à± à°°à±†à°¡à±à°¡à°¿, à°®à°¾à°²à°•à±Šà°‚à°¡à°¯à±à°¯, à°•à±‡à°µà±€ à°°à°®à°£à°¾à°°à±†à°¡à±à°¡à°¿, à°¶à±à°°à±€à°¨à°¿à°µà°¾à°¸à±, à°¨à°¾à°—à°¶à±‡à°–à°°à±, à°°à°˜à±à°¨à°¾à°§à± à°—à±Œà°¡à±, à°•à±ƒà°·à±à°£, à°†à°‚à°œà°¨à±‡à°¯à±à°²à±, à°¶à±à°°à±€à°¨à°¿à°µà°¾à°¸à± à°—à±Œà°¡à±, à°¶à±‡à°·à°¯à±à°¯, à°ªà°¾à°·à°¾, à°¶à±‹à°­, à°¶à±à°°à±€à°¨à°¿à°µà°¾à°¸à±, à°®à°§à±à°•à°°à±, à°¸à±ˆà°¦à±à°²à±, à°•à±Šà°‚à°¡à°¾ à°®à°§à±, à°šà°‚à°¦à±à°°à°¶à±‡à°–à°°à± à°°à±†à°¡à±à°¡à°¿, à°¶à±à°°à±€à°¨à°¿à°µà°¾à°¸à± à°°à±†à°¡à±à°¡à°¿, à°¯à°¾à°®à°¿à°¨à°¿, à°¶à±à°°à±€à°¨à°¿à°µà°¾à°¸à±, à°¨à°¾à°°à°¾à°¯à°£ à°¸à±à°µà°¾à°®à°¿, à°°à°¾à°œà±, à°°à°¾à°®à± à°®à±‹à°¹à°¨à±, à°¶à°¿à°µà°¦à°¾à°¸à±, à°¦à±‚à°¦à°¿, à°¸à±à°§à°¾à°•à°°à±, à°•à±à°²à±à°¦à±€à°ªà±, à°¨à°¾à°°à°¯, à°µà°¿à°œà°¯à± , à°¶à±à°°à±€à°¨à°¿à°µà°¾à°¸à±, à°¶à±€à°·à± à°°à±†à°¡à±à°¡à°¿, à°ªà±à°°à°µà±€à°£à± à°•à±à°®à°¾à°°à±
--------------------------------------------------

Sample 2/3:
--------------------------------------------------
Generated: à°œà°—à°¨à± à°®à±‹à°¹à°¨à± à°°à±†à°¡à±à°¡à°¿à°¨à°¿ à°šà°‚à°¦à±à°°à°¬à°¾à°¬à± à°ªà±à°°à°¶à±à°¨à°¿à°‚à°šà°¾à°°à± à°¬à±à°§à°µà°¾à°°à°‚ à°µà°¿à°œà°¯à°µà°¾à°¡à°²à±‹à°¨à°¿ à°šà°‚à°¦à±à°°à°¬à°¾à°¬à± à°¨à°¿à°µà°¾à°¸à°‚à°²à±‹ à°†à°¯à°¨ à°µà°¿à°²à±‡à°•à°°à±à°²à°¤à±‹ à°®à°¾à°Ÿà±à°²à°¾à°¡à°¾à°°à± à°ªà±à°°à°¤à±à°¯à±‡à°• à°¹à±‹à°¦à°¾à°ªà±ˆ à°°à°¾à°œà°•à±€à°¯ à°ªà±‹à°°à°¾à°Ÿà°‚ à°šà±‡à°¸à±à°¤à±‡ à°šà°‚à°¦à±à°°à°¬à°¾à°¬à± à°šà±‡à°¤à°•à°¾à°¨à°¿à°µà°¾à°¡à°¨à°¿ à°®à°‚à°¡à°¿à°ªà°¡à±à°¡à°¾à°°à± à°…à°¨à±à°¨à°¿ à°ªà°¾à°°à±à°Ÿà±€à°²à± à°¹à±‹à°¦à°¾ à°•à±‹à°¸à°‚ à°ªà±‹à°°à°¾à°Ÿà°‚ à°šà±‡à°¸à±à°¤à°¾à°¯à°¨à±à°¨à°¾à°°à± à°¹à±‹à°¦à°¾ à°•à±‹à°¸à°‚ à°ªà±‹à°°à°¾à°¡à±à°¤à±à°¨à±à°¨ à°¤à±†à°²à±à°—à±à°¦à±‡à°¶à°‚ à°ªà°¾à°°à±à°Ÿà±€, à°‡à°ªà±à°ªà±à°¡à± à°¹à±‹à°¦à°¾ à°•à±‹à°¸à°‚ à°ªà±‹à°°à°¾à°¡à±à°¤à±‹à°‚à°¦à°¨à°¿, à°ˆ à°ªà±‹à°°à°¾à°Ÿà°‚ à°µà°²à±à°² à°šà°‚à°¦à±à°°à°¬à°¾à°¬à± à°¨à±€à°¤à°¿, à°¨à°¿à°œà°¾à°¯à°¿à°¤à±€, à°¨à°¿à°œà°¾à°¯à°¿à°¤à±€ à°²à±‡à°¨à°¿ à°®à°¨à°¿à°·à°¿à°—à°¾ à°®à°¿à°—à°¿à°²à°¿à°ªà±‹à°¯à°¾à°¡à°¨à°¿ à°Žà°¦à±à°¦à±‡à°µà°¾ à°šà±‡à°¶à°¾à°°à± à°¬à°¾à°¬à± à°•à±à°Ÿà±à°‚à°¬ à°¸à°­à±à°¯à±à°²à± à°¤à°¾à°¨à± à°µà°šà±à°šà°¿ à°šà°‚à°¦à±à°°à°¬à°¾à°¬à±à°¨à°¿ à°•à°²à°¿à°¸à°¿ à°¨à°¿à°°à°¸à°¨ à°¤à±†à°²à°¿à°ªà°¿, à°®à±à°‚à°¦à±à°•à±Šà°šà±à°šà°¿ à°¨à°¿à°°à°¸à°¨ à°¤à±†à°²à°¿à°ªà°¿à°¤à±‡ à°šà°‚à°¦à±à°°à°¬à°¾à°¬à± à°¸à±à°ªà°‚à°¦à°¿à°‚à°šà°²à±‡à°¦à°¨à±à°¨à°¾à°°à± à°šà°‚à°¦à±à°°à°¬à°¾à°¬à± à°šà±‡à°¤à°•à°¾à°¨à°¿à°¤à°¨à°¾à°¨à°¿à°•à°¿ à°šà°‚à°¦à±à°°à°¬à°¾à°¬à± à°¤à±€à°°à±
--------------------------------------------------

Sample 3/3:
--------------------------------------------------
Generated: à°œà°—à°¨à± à°®à±‹à°¹à°¨à± à°°à±†à°¡à±à°¡à°¿, à°•à°¾à°•à°¾à°¨à°¿ à°—à±‹à°µà°°à±à°¦à°¨à± à°°à±†à°¡à±à°¡à°¿, à°•à±‡à°µà±€à°Žà°‚ à°ªà±à°°à°¸à°¾à°¦à±, à°°à°¾à°·à±à°Ÿà±à°° à°ªà°°à±à°¯à°¾à°Ÿà°• à°¶à°¾à°– à°®à°‚à°¤à±à°°à°¿ à°¶à±à°°à±€à°¨à°¿à°µà°¾à°¸ à°°à°¾à°µà±, à°à°ªà±€ à°ªà°°à±à°¯à°¾à°Ÿà°• à°¶à°¾à°– à°®à°‚à°¤à±à°°à°¿ à°¨à°¾à°°à°¾ à°²à±‹à°•à±‡à°·à±, à°®à°‚à°¤à±à°°à°¿ à°¬à°¾à°²à°¿à°¨à±‡à°¨à°¿ à°¶à±à°°à±€à°¨à°¿à°µà°¾à°¸ à°°à±†à°¡à±à°¡à°¿à°²à± à°šà°¿à°¨à°¬à°¾à°¬à±, à°²à±‹à°•à±‡à°·à± à°²à± à°ªà°²à±à°µà±à°°à± à°°à°¾à°œà°•à±€à°¯ à°¨à°¾à°¯à°•à±à°²à±, à°…à°§à°¿à°•à°¾à°°à±à°²à±, à°®à°‚à°¤à±à°°à±à°²à±, à°Žà°®à±à°®à±†à°²à±à°¯à±‡à°²à±, à°Žà°‚à°ªà±€à°²à±, à°¨à±‡à°¤à°²à± à°ˆ à°¸à°®à°¾à°µà±‡à°¶à°‚à°²à±‹ à°ªà°¾à°²à±à°—à±Šà°¨à±à°¨à°¾à°°à± à°ˆ à°¨à±‡à°ªà°¥à±à°¯à°‚à°²à±‹ à°‡à°ªà±à°ªà±à°¡à± à°šà°¿à°¨à°¬à°¾à°¬à±, à°²à±‹à°•à±‡à°·à± à°²à± à°•à±‚à°¡à°¾ à°šà°¿à°¨à°¬à°¾à°¬à±à°¨à± à°•à°²à°¿à°¶à°¾à°°à± à°¬à±€à°œà±‡à°ªà±€, à°œà°¨à°¸à±‡à°¨ à°¨à±‡à°¤à°²à± à°•à±‚à°¡à°¾ à°µà°šà±à°šà±‡ à°Žà°¨à±à°¨à°¿à°•à°²à±à°²à±‹ à°šà°¿à°¨à°¬à°¾à°¬à± à°¬à±€à°œà±‡à°ªà±€à°¨à°¿ à°µà°¦à°¿à°²à±‡à°¸à±à°¤à±‡ à°¬à°¾à°—à±à°‚à°Ÿà±à°‚à°¦à°¨à°¿ à°œà°—à°¨à± à°­à°¾à°µà°¿à°¸à±à°¤à±à°¨à±à°¨à°Ÿà±à°²à± à°¤à±†à°²à±à°¸à±à°¤à±‹à°‚à°¦à°¿ à°®à°°à±‹à°µà±ˆà°ªà±
--------------------------------------------------
```
#### 10M, 1/10 of CulturaX 'te' data:
```
Generating 3 samples with 100 tokens each...
Optimized for A4000 GPU
================================================================================

Sample 1/3:
--------------------------------------------------
Generated: à°¨à°®à°¸à±à°•à°¾à°°à°‚, à°¨à±‡à°¨à± à°¤à±†à°²à±à°—à±-] à°‡à°‚à°¦à±à°²à±‹ à°šà±†à°‚à°¦à°¿à°¨ à°ªà±†à°¦à±à°¦ à°¸à°¿à°¨à°¿à°®à°¾à°•à± 10] 141 à°œà±à°¤à±‹ 3-3à°²à±‹ 24] 102-2 - à°šà°¾à°²à°¾ à°ªà±à°°à°ªà°‚à°š à°Žà°•à±à°•à±à°µà°²à±à°²à°ªà±ˆ à°¡à±†à°¨à±à°¡à°¿à°²à± 1] 19-0 à°¨à±à°‚à°šà°¿ à°Ÿà±€à°¡à±€à°ªà±€ 125-à°Ÿà±€ 15, 20 15à°¨ | 18 1/40, 1813- à°ªà±‹à°¯à°¿ à°ˆ à°¨à±†à°², 163, à°‡à°¦à°¿
--------------------------------------------------

Sample 2/3:
--------------------------------------------------
Generated: à°¨à°®à°¸à±à°•à°¾à°°à°‚, à°¨à±‡à°¨à± à°¤à±†à°²à±à°—à±à°•à°¿, à°‡à°¦à°¿ à°¦à± "à°°à±†à°ªà± à°¯à±Šà°•à±à°• à°ˆ à°¤à°°à±à°µà°¾à°¤ 51-1 à°¶à°¾à°¤à°‚ à°®à°‚à°¦à°¿ à°ˆ à°•à± 20 à°µà°°à°•à± 040, à°•à°¡à±à°¸à°¿) 19 à°‡à°• 30140196à°µà°£à°‚ à°¦à±€à°¨à°¿à°ªà±ˆ à°œà°¿à°²à±à°²à°¾ 2, [2550010222462104400 [1040003013] 6 : à°à°ªà°¿à°¸à±
--------------------------------------------------

Sample 3/3:
--------------------------------------------------
Generated: à°¨à°®à°¸à±à°•à°¾à°°à°‚, à°¨à±‡à°¨à± à°¤à±†à°²à±à°—à± à°°à±‹à°œà± in |] à°¸à± à°…à°²à± | 1)à° | march | 20 à°®à°¿à°²à°¿à°¯à°¨à±à°²à± 54020/ à°°à±‚ à°œà°¿à°²à±à°²à°¾ à°¸à°‚à°¸à±à°¥à°²à±‹à°¨à°¿ 150015 à°²à°•à±à°·à°² à°ªà±ˆ 18559-17: 1520190-20195 pm à°¨à±à°‚à°¡à°¿ oneindia 22013 pm 12027, 19:5, |90
--------------------------------------------------

================================================================================
A4000 Optimized Sampling Complete!
Final GPU memory: 0.42 GB
blockchain@amritesh-precision-3660:~/Insolare-LLM$ python3 sample.py
A4000 GPU Optimized Sampling
Using device: cuda
Using dtype: float16
Loading tokenizer from 10M.model...
Tokenizer loaded with vocab size: 16768
Loading checkpoint from out/trial_checkpoint.pth...
Checkpoint loaded to CPU
Model config: {'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'block_size': 512, 'bias': False, 'dropout': 0.0, 'vocab_size': 16768}
Number of parameters:  98224896
Model created with 98,224,896 parameters
Model weights loaded successfully!
Moving model to cuda...
GPU memory allocated: 0.41 GB
GPU memory cached: 0.44 GB

Prompt: à°¨à°®à°¸à±à°•à°¾à°°à°‚, à°¨à±‡à°¨à± à°¤à±†à°²à±à°—à±
Encoded prompt: [13718, 265, 371, 416]
Prompt length: 4 tokens

Generating 3 samples with 100 tokens each...
Optimized for A4000 GPU
================================================================================

Sample 1/3:
--------------------------------------------------
Generated: à°¨à°®à°¸à±à°•à°¾à°°à°‚, à°¨à±‡à°¨à± à°¤à±†à°²à±à°—à± à°†à°°à±à°¥à°¿à°• à°°à±‹à°œà±à°²à± à°µà°¿à°§à°¾à°²à±à°¸à°¿à°µà°¿à°¸à±à°¤à±‹à°‚à°¦à°¿ à°† à°¤à±‹ à°‰à°‚à°¦à°¿ à°…à°¯à°¿à°¨à°ªà±à°ªà°Ÿà°¿à°•à±€ à°…à°¨à±à°¨ â€™ à°…à°¨à±‡ à°œà°‚à°Ÿà°²à±‹à°¨à°¿ â€˜à°¸à±à°¤à±à°¸à± 15 à°®à°‚à°¦à°¿à°•à°¿ à°¨à°¾à°•à± à°®à±‚à°¡à± |19 à°‰à°¨à±à°¨à°¾à°¯à°¿ 16 à°¨à±à°‚à°šà°¿ 120] telugu [0-213) â€“13 à°²à±‹ à°† à°šà°¿à°¤à±à°°à°‚ 144 à°¨à±à°‚à°šà°¿ à°•à±‚à°¡à°¾ 2, 3205 à°¨à±à°‚à°šà°¿ 20190008348
--------------------------------------------------

Sample 2/3:
--------------------------------------------------
Generated: à°¨à°®à°¸à±à°•à°¾à°°à°‚, à°¨à±‡à°¨à± à°¤à±†à°²à±à°—à± à°œà±€à°µà°¿à°¤à°¾à°¨à±à°¨à°¿ à°¨à°¾ 13 à°µà°¸à°¿ 2, à°•à± 1, 1: à°Žà°¸à± 3 - 08 à°•à°°à±‹à°¨à°¾ à°²à±‹ à°•à°²à°¿à°¸à°¿ à°† à°®à°§à±à°¯ à°¤à°¨à°¦à°¿ jle 13015-20 à°¨à±à°‚à°¡à°¿ 10] 149à°¨à±‹à°—à°¾ à°®à°¾à°¤à±à°°à°®à±‡ | 30-3, 113 à°²à±‹ à°“ à°°à±†à°‚à°¡à± à°ªà±à°°à±ˆà°µà±‡à°Ÿà± à°¤à°²à±à°²à°¿à°•à± à°¤à°¿à°°à°¿à°—à°¿ - à°²à±, à°‰à°ªà±à°ªà±, 198-2206:15
--------------------------------------------------

Sample 3/3:
--------------------------------------------------
Generated: à°¨à°®à°¸à±à°•à°¾à°°à°‚, à°¨à±‡à°¨à± à°¤à±†à°²à±à°—à± à°¦à±‡à°¶à°‚à°¾à°œà±€à°•à± à°‰à°‚à°¦à°¿, à°•à±‚à°¡à°¾ à°®à°¾à°Ÿà±à°²à°¾à°¡à±à°•à±à°¨à°¿ à°‰à°¨à±à°¨ à°¸à°®à°¯à°‚à°²à±‹ à°ˆ à°°à°¾à°·à±à°Ÿà±à°° à°°à°•à°¾à°² à°Žà°•à±à°•à±à°µà°—à°¾ à°…à°¨à°¿ à°…à°¡à±à°—à±à°•à± à°•à±‚à°¡à°¾ à°‰à°¨à±à°¨ à°­à°¾à°µà°‚à°²à°¨à± à°ˆ à°°à±‹à°œà±, à°† "à°•à±‡ 20018-48 à°¨à±‡à°¨à± à° à°²à±‹, à°Žà°•à±à°•à±à°µ à°…à°¨à±‡ à°•à± à°•à±‡à°‚à°¦à±à°° 100 à°²à°•à±à°·à°² 3 hid news 19215, 1020 1, 3 3 à°®à°‚à°¦à°¿ telugu: 158993] 14à°µà±‡à°² à°…à°¨à°¿ à°‰à°¨à±à°¨ à°°à±‹à°œà±à°² -
--------------------------------------------------

```

### Useful commands
* `python3 download.py`
* `nohup python3 prepare_data.py > /dev/nullÂ 2>&1Â &`
* `nohup python3 train_tokenizer.py --input_file telugu_training_data_full.txt --model_prefix 16768_full_txt --vocab_size 16768 > tokenizer_16768.logÂ 2>&1Â &`
* `python3 txt_to_bin.py`
* `nohup python3 train.py > train.log 2>&1`

### BTS (behind the scenes)
* DOT stands for Decoder-only-Transformer, a funny pun intended for a dialogue in Shanker's Robo (2010) movie.
* I recently trained the next version of Buggi (an SLM on Telugu text): 10M, 98M, and 124M parameter model variantsÂ (a modified GPT-2 architecture) on CulturaX's Telugu collection. Learnt a lot about processing raw text, training a tokeniser, training dynamics, loss functions, hyperparameters, and GPUs at a relatively biggerÂ scale than Buggi-1. Ran a fewÂ experiments on a smaller variant and a subset of data, then gradually scaled it up, and experienced the capability growÂ from generating short, valid words to sentences to paragraphs. Turns out, with good textual data, transformer-basedÂ architecture, tokeniser, compute, one could just train a model to memorise/encode any data (kinda by heart) as its parameters. Since a lot of data is news articles about movies, mostly, a noun is contextualised as a movie name by the model ðŸ¤£, for ex, Amaravati (Andhra Pradesh capital) is confused as a movie name when I asked about its progress. Will host it on GCP if I get to use GCP's 300$ credit with GPU for sometime. See sample results: github.com/Vasudeva-bit/buggi - posted on @KilaruVasudeva

### More data for Telugu
* Swetcha (HYD NGO), AI4Bharat (IIT-Madras)
* A few other: LLM Labs, Anuskha/Anusha Kaggle Telugu Books etc...
